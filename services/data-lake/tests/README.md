# Data Lake Integration Tests

This directory contains integration tests for the data lake service.

## Overview

Integration tests in this directory automatically manage their own Docker containers using pytest fixtures. You don't need to manually start/stop services - the tests handle this for you.

## Prerequisites

1.  **Generate Environment Files**: The tests use credentials from `.env` files generated by the unified setup script. From the **project root directory**, run:

    ```bash
    ./setup-all-services.sh
    ```

    This creates `services/data-lake/.env` with MinIO credentials and configuration.

2.  **Install Dependencies**: From the **project root**, with virtual environment activated:

    ```bash
    source .venv/bin/activate
    pip install -r services/data-lake/requirements.txt
    ```

## Running the Tests

**From the project root**, with the virtual environment activated:

```bash
# Run all data-lake tests (including integration tests)
./run-tests.sh data-lake

# Run with verbose output
./run-tests.sh data-lake -v

# Run specific test file
cd services/data-lake
pytest tests/test_data_lake_integration.py -v
```

**What happens during test execution:**
1. Pytest fixtures automatically start required Docker containers (MinIO)
2. Tests run against the containerized services
3. Containers are automatically stopped and cleaned up after tests complete

**Note**: Integration tests manage their own Docker containers via the `docker_services` fixture, so you don't need to manually start `docker compose` before running tests.
