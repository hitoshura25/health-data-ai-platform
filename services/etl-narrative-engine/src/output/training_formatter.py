"""
Training data formatter for AI model fine-tuning.

Transforms clinical narratives into JSONL training examples with instruction-output pairs.
Module 4 implementation as per specs/module-4-training-data-output.md
"""

import hashlib
import json
from datetime import UTC, datetime
from typing import Any

import structlog

logger = structlog.get_logger()


class TrainingDataFormatter:
    """
    Formatter for AI training data output.

    Generates JSONL (JSON Lines) files with instruction-output pairs for model training.
    Organizes output by health domain and uploads to S3 with intelligent naming.

    Usage:
        formatter = TrainingDataFormatter(s3_client, 'health-data', 'training/')
        success = await formatter.generate_training_output(
            narrative="Blood glucose data shows...",
            source_metadata={...},
            processing_metadata={...}
        )
    """

    def __init__(
        self,
        s3_client,
        bucket_name: str,
        training_prefix: str = "training/",
        include_metadata: bool = True
    ):
        """
        Initialize training data formatter.

        Args:
            s3_client: aioboto3 S3 client for uploads
            bucket_name: S3 bucket name for training data
            training_prefix: S3 prefix for training files (default: 'training/')
            include_metadata: Include metadata in JSONL output (default: True)
        """
        self.s3_client = s3_client
        self.bucket_name = bucket_name
        self.training_prefix = training_prefix.rstrip('/') + '/'
        self.include_metadata = include_metadata
        self.logger = structlog.get_logger()

        # Map record types to health domains for organization
        self.domain_mapping = {
            'BloodGlucoseRecord': 'metabolic_diabetes',
            'HeartRateRecord': 'cardiovascular_fitness',
            'SleepSessionRecord': 'sleep_wellness',
            'StepsRecord': 'physical_activity',
            'ActiveCaloriesBurnedRecord': 'physical_activity',
            'HeartRateVariabilityRmssdRecord': 'cardiovascular_fitness',
        }

        # Instruction templates per record type
        self.instruction_templates = {
            'BloodGlucoseRecord': (
                "Analyze this blood glucose data and provide clinical insights.",
                "Blood glucose data including fasting, post-meal, and overnight readings."
            ),
            'HeartRateRecord': (
                "Analyze this heart rate data and provide cardiovascular insights.",
                "Heart rate measurements including resting, active, and exercise sessions."
            ),
            'SleepSessionRecord': (
                "Analyze this sleep data and provide sleep quality insights.",
                "Sleep session data including duration, timing, and sleep stages."
            ),
            'StepsRecord': (
                "Analyze this daily activity data and provide fitness insights.",
                "Daily step count data showing activity patterns."
            ),
            'ActiveCaloriesBurnedRecord': (
                "Analyze this calorie burn data and provide activity insights.",
                "Active calories burned during exercise and daily activities."
            ),
            'HeartRateVariabilityRmssdRecord': (
                "Analyze this heart rate variability data and provide recovery insights.",
                "HRV RMSSD measurements indicating cardiovascular fitness and recovery."
            ),
        }

    async def generate_training_output(
        self,
        narrative: str,
        source_metadata: dict[str, Any],
        processing_metadata: dict[str, Any]
    ) -> bool:
        """
        Generate training data entry and append to JSONL file.

        Args:
            narrative: Clinical narrative generated by processor
            source_metadata: Original message metadata containing:
                - bucket: Source S3 bucket
                - key: Source S3 key
                - record_type: Health record type
                - user_id: User identifier
                - correlation_id: Request correlation ID
            processing_metadata: Processing results containing:
                - duration: Processing time in seconds
                - quality_score: Data quality score (0.0-1.0)
                - record_count: Number of records processed
                - clinical_insights: Structured clinical insights dict

        Returns:
            True if successful, False otherwise
        """
        try:
            # Validate inputs
            if not narrative or not narrative.strip():
                self.logger.warning("empty_narrative", source_key=source_metadata.get('key'))
                return False

            record_type = source_metadata.get('record_type', 'Unknown')

            # Generate instruction and input text
            instruction, input_text = self._generate_instruction_input(
                record_type,
                processing_metadata
            )

            # Create training example
            training_example = {
                'instruction': instruction,
                'input': input_text,
                'output': narrative,
            }

            # Add metadata if enabled
            if self.include_metadata:
                training_example['metadata'] = {
                    'record_type': record_type,
                    'user_id': source_metadata.get('user_id'),
                    'correlation_id': source_metadata.get('correlation_id'),
                    'processing_timestamp': datetime.now(UTC).isoformat(),
                    'source_bucket': source_metadata.get('bucket'),
                    'source_key': source_metadata.get('key'),
                    'quality_score': processing_metadata.get('quality_score', 0.0),
                    'record_count': processing_metadata.get('record_count', 0),
                    'processing_duration_seconds': processing_metadata.get('duration', 0.0),
                    'clinical_insights': processing_metadata.get('clinical_insights', {}),
                    'health_domain': self._get_health_domain(record_type),
                }

            # Generate JSONL line (single line JSON)
            jsonl_line = json.dumps(training_example, ensure_ascii=False) + '\n'

            # Determine S3 key for training file
            s3_key = self._generate_training_file_key(record_type)

            # Append to JSONL file in S3
            await self._append_to_jsonl_file(s3_key, jsonl_line)

            self.logger.info(
                "training_output_generated",
                record_type=record_type,
                s3_key=s3_key,
                narrative_length=len(narrative)
            )

            return True

        except Exception as e:
            self.logger.error(
                "training_output_failed",
                error=str(e),
                record_type=source_metadata.get('record_type'),
                source_key=source_metadata.get('key')
            )
            return False

    def _generate_instruction_input(
        self,
        record_type: str,
        processing_metadata: dict[str, Any]
    ) -> tuple[str, str]:
        """
        Generate instruction and input text based on record type.

        Args:
            record_type: Health record type
            processing_metadata: Processing metadata for context

        Returns:
            Tuple of (instruction, input_text)
        """
        # Default instruction and input
        default = (
            "Analyze this health data and provide clinical insights.",
            "Health data measurements from mobile health tracking."
        )

        instruction, input_template = self.instruction_templates.get(record_type, default)

        # Enhance input with summary stats if available
        clinical_insights = processing_metadata.get('clinical_insights', {})
        if clinical_insights:
            # Try to get total readings/samples count
            total_readings = clinical_insights.get(
                'total_readings',
                clinical_insights.get('total_samples', 0)
            )
            if total_readings:
                input_text = f"{input_template} Total measurements: {total_readings}."
            else:
                input_text = input_template
        else:
            input_text = input_template

        return instruction, input_text

    def _get_health_domain(self, record_type: str) -> str:
        """
        Map record type to health domain for file organization.

        Args:
            record_type: Health record type

        Returns:
            Health domain string (e.g., 'metabolic_diabetes')
        """
        return self.domain_mapping.get(record_type, 'general_health')

    def _generate_training_file_key(self, record_type: str) -> str:
        """
        Generate S3 key for training JSONL file.

        Format: training/{domain}/{year}/{month}/health_journal_{year}_{month}.jsonl

        Args:
            record_type: Health record type

        Returns:
            S3 key for training file
        """
        health_domain = self._get_health_domain(record_type)
        now = datetime.now(UTC)

        # Monthly JSONL files organized by domain
        key = (
            f"{self.training_prefix}{health_domain}/"
            f"{now.year}/"
            f"{now.month:02d}/"
            f"health_journal_{now.year}_{now.month:02d}.jsonl"
        )

        return key

    async def _append_to_jsonl_file(self, s3_key: str, jsonl_line: str) -> None:
        """
        Append JSONL line to existing file or create new file.

        Note: S3 doesn't support true append, so we download, append, and re-upload.
        For high-throughput scenarios, consider batching or local buffering.

        Args:
            s3_key: S3 key for training file
            jsonl_line: JSONL line to append (must end with newline)

        Raises:
            Exception: If S3 operations fail
        """
        try:
            # Try to download existing file
            response = await self.s3_client.get_object(
                Bucket=self.bucket_name,
                Key=s3_key
            )

            # Read existing content
            async with response['Body'] as stream:
                existing_content = await stream.read()

            # Append new line
            new_content = existing_content + jsonl_line.encode('utf-8')

            self.logger.debug(
                "appending_to_existing_jsonl",
                s3_key=s3_key,
                existing_size=len(existing_content),
                new_size=len(new_content)
            )

        except self.s3_client.exceptions.NoSuchKey:
            # File doesn't exist, create new
            new_content = jsonl_line.encode('utf-8')

            self.logger.debug(
                "creating_new_jsonl_file",
                s3_key=s3_key,
                size=len(new_content)
            )

        except Exception as e:
            raise Exception(f"Failed to read existing JSONL file: {str(e)}") from e

        # Upload updated file
        await self.s3_client.put_object(
            Bucket=self.bucket_name,
            Key=s3_key,
            Body=new_content,
            ContentType='application/jsonl',
            Metadata={
                'module': 'etl-narrative-engine',
                'component': 'training-data-output',
                'format': 'jsonl'
            }
        )

        self.logger.debug(
            "jsonl_file_uploaded",
            s3_key=s3_key,
            size=len(new_content)
        )

    def generate_content_hash(self, narrative: str, source_key: str) -> str:
        """
        Generate unique hash for training example to support deduplication.

        Args:
            narrative: Clinical narrative text
            source_key: Source S3 key

        Returns:
            SHA-256 hash hex string
        """
        content = f"{narrative}::{source_key}"
        return hashlib.sha256(content.encode('utf-8')).hexdigest()
