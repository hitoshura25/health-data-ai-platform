# Module 4: Training Data Output

**Module ID:** ETL-M4
**Priority:** P1 (Core Output Module)
**Estimated Effort:** 1 week
**Dependencies:** Module 1 (Core Consumer calls this), Module 3 (Provides narratives)
**Team Assignment:** Backend Developer with AI/ML Knowledge

---

## Module Overview

This module transforms clinical narratives generated by Module 3 processors into training data for AI model fine-tuning. It generates JSONL (JSON Lines) files with instruction-output pairs, enriched with metadata, and uploads them to MinIO/S3 for use in model training pipelines.

### Key Responsibilities
- Format narratives as JSONL training examples
- Generate contextual instructions for each record type
- Add metadata (timestamp, data quality, clinical insights)
- Organize by health domain (metabolic, cardiovascular, sleep, activity)
- Upload to S3 `training/` prefix with intelligent naming
- Handle deduplication to prevent duplicate training examples

### What This Module Does NOT Include
- ❌ Message consumption (Module 1)
- ❌ Clinical processing (Module 3)
- ❌ Model training (separate service)
- ❌ Metrics collection (Module 5)

---

## JSONL Training Data Format

### Format Specification

```json
{
  "instruction": "Analyze this blood glucose data and provide clinical insights.",
  "input": "Blood glucose readings: 95 mg/dL (fasting), 142 mg/dL (post-meal), 88 mg/dL (bedtime). Total: 450 readings over 30 days.",
  "output": "Blood glucose data shows 450 readings over a 30-day period with mean glucose of 142.3 mg/dL. Glucose control is moderate with variability (CV 38%) and 62% time in target range...",
  "metadata": {
    "record_type": "BloodGlucoseRecord",
    "user_id": "user123",
    "correlation_id": "abc-def-123",
    "processing_timestamp": "2025-11-15T10:30:00Z",
    "source_bucket": "health-data",
    "source_key": "raw/BloodGlucoseRecord/2025/11/15/user123_1731628800_abc123.avro",
    "quality_score": 0.95,
    "record_count": 450,
    "processing_duration_seconds": 2.3,
    "clinical_insights": {
      "control_status": "fair",
      "hypoglycemic_events": 3,
      "hyperglycemic_events": 45
    },
    "health_domain": "metabolic_diabetes"
  }
}
```

### S3 Storage Structure

```
training/
├── metabolic_diabetes/
│   └── 2025/
│       └── 11/
│           └── health_journal_2025_11.jsonl
│
├── cardiovascular_fitness/
│   └── 2025/
│       └── 11/
│           └── health_journal_2025_11.jsonl
│
├── sleep_wellness/
│   └── 2025/
│       └── 11/
│           └── health_journal_2025_11.jsonl
│
└── physical_activity/
    └── 2025/
        └── 11/
            └── health_journal_2025_11.jsonl
```

---

## Interface Implementation

### **1. Training Data Formatter Interface**

```python
# src/output/training_formatter.py
from typing import Dict, Any
from datetime import datetime
import json
import hashlib
from pathlib import Path

class TrainingDataFormatter:
    """Formatter for AI training data output"""

    def __init__(self, s3_client, bucket_name: str, training_prefix: str = "training/"):
        """
        Initialize training data formatter

        Args:
            s3_client: aioboto3 S3 client
            bucket_name: S3 bucket for training data
            training_prefix: S3 prefix for training files
        """
        self.s3_client = s3_client
        self.bucket_name = bucket_name
        self.training_prefix = training_prefix

        # Map record types to health domains
        self.domain_mapping = {
            'BloodGlucoseRecord': 'metabolic_diabetes',
            'HeartRateRecord': 'cardiovascular_fitness',
            'SleepSessionRecord': 'sleep_wellness',
            'StepsRecord': 'physical_activity',
            'ActiveCaloriesBurnedRecord': 'physical_activity',
            'HeartRateVariabilityRmssdRecord': 'cardiovascular_fitness',
        }

    async def generate_training_output(
        self,
        narrative: str,
        source_metadata: Dict[str, Any],
        processing_metadata: Dict[str, Any]
    ) -> bool:
        """
        Generate training data entry and append to JSONL file

        Args:
            narrative: Clinical narrative generated by processor
            source_metadata: Original message metadata (bucket, key, user_id, etc.)
            processing_metadata: Processing results (duration, quality_score, clinical_insights)

        Returns:
            True if successful, False otherwise
        """
        try:
            # Generate instruction and input
            instruction, input_text = self._generate_instruction_input(
                source_metadata['record_type'],
                processing_metadata
            )

            # Create training example
            training_example = {
                'instruction': instruction,
                'input': input_text,
                'output': narrative,
                'metadata': {
                    'record_type': source_metadata['record_type'],
                    'user_id': source_metadata['user_id'],
                    'correlation_id': source_metadata['correlation_id'],
                    'processing_timestamp': datetime.utcnow().isoformat() + 'Z',
                    'source_bucket': source_metadata['bucket'],
                    'source_key': source_metadata['key'],
                    'quality_score': processing_metadata.get('quality_score', 0.0),
                    'record_count': processing_metadata.get('record_count', 0),
                    'processing_duration_seconds': processing_metadata.get('duration', 0.0),
                    'clinical_insights': processing_metadata.get('clinical_insights', {}),
                    'health_domain': self._get_health_domain(source_metadata['record_type']),
                }
            }

            # Generate JSONL entry
            jsonl_line = json.dumps(training_example) + '\n'

            # Determine S3 key
            s3_key = self._generate_training_file_key(
                source_metadata['record_type']
            )

            # Append to JSONL file in S3
            await self._append_to_jsonl_file(s3_key, jsonl_line)

            return True

        except Exception as e:
            # Log error
            return False

    def _generate_instruction_input(
        self,
        record_type: str,
        processing_metadata: Dict[str, Any]
    ) -> tuple[str, str]:
        """Generate instruction and input text based on record type"""

        instructions = {
            'BloodGlucoseRecord': (
                "Analyze this blood glucose data and provide clinical insights.",
                "Blood glucose data including fasting, post-meal, and overnight readings."
            ),
            'HeartRateRecord': (
                "Analyze this heart rate data and provide cardiovascular insights.",
                "Heart rate measurements including resting, active, and exercise sessions."
            ),
            'SleepSessionRecord': (
                "Analyze this sleep data and provide sleep quality insights.",
                "Sleep session data including duration, timing, and sleep stages."
            ),
            'StepsRecord': (
                "Analyze this daily activity data and provide fitness insights.",
                "Daily step count data showing activity patterns."
            ),
            'ActiveCaloriesBurnedRecord': (
                "Analyze this calorie burn data and provide activity insights.",
                "Active calories burned during exercise and daily activities."
            ),
            'HeartRateVariabilityRmssdRecord': (
                "Analyze this heart rate variability data and provide recovery insights.",
                "HRV RMSSD measurements indicating cardiovascular fitness and recovery."
            ),
        }

        default = (
            "Analyze this health data and provide clinical insights.",
            "Health data measurements from mobile health tracking."
        )

        instruction, input_template = instructions.get(record_type, default)

        # Enhance input with summary stats if available
        clinical_insights = processing_metadata.get('clinical_insights', {})
        if clinical_insights:
            total_readings = clinical_insights.get('total_readings', clinical_insights.get('total_samples', 0))
            if total_readings:
                input_text = f"{input_template} Total measurements: {total_readings}."
            else:
                input_text = input_template
        else:
            input_text = input_template

        return instruction, input_text

    def _get_health_domain(self, record_type: str) -> str:
        """Map record type to health domain"""
        return self.domain_mapping.get(record_type, 'general_health')

    def _generate_training_file_key(self, record_type: str) -> str:
        """Generate S3 key for training JSONL file"""

        health_domain = self._get_health_domain(record_type)
        now = datetime.utcnow()

        # Format: training/{domain}/{year}/{month}/health_journal_{year}_{month}.jsonl
        key = (
            f"{self.training_prefix}{health_domain}/"
            f"{now.year}/"
            f"{now.month:02d}/"
            f"health_journal_{now.year}_{now.month:02d}.jsonl"
        )

        return key

    async def _append_to_jsonl_file(self, s3_key: str, jsonl_line: str) -> None:
        """Append JSONL line to existing file or create new file"""

        try:
            # Try to download existing file
            response = await self.s3_client.get_object(
                Bucket=self.bucket_name,
                Key=s3_key
            )

            # Read existing content
            async with response['Body'] as stream:
                existing_content = await stream.read()

            # Append new line
            new_content = existing_content + jsonl_line.encode('utf-8')

        except self.s3_client.exceptions.NoSuchKey:
            # File doesn't exist, create new
            new_content = jsonl_line.encode('utf-8')

        except Exception as e:
            raise Exception(f"Failed to read existing JSONL file: {str(e)}")

        # Upload updated file
        await self.s3_client.put_object(
            Bucket=self.bucket_name,
            Key=s3_key,
            Body=new_content,
            ContentType='application/jsonl'
        )
```

---

## Deduplication

### Preventing Duplicate Training Examples

```python
# src/output/training_deduplicator.py
import hashlib
import json
from typing import Set

class TrainingDeduplicator:
    """Prevent duplicate training examples"""

    def __init__(self, dedup_store):
        """
        Initialize deduplicator

        Args:
            dedup_store: SQLite or Redis store for tracking processed entries
        """
        self.dedup_store = dedup_store

    def generate_content_hash(self, narrative: str, source_key: str) -> str:
        """Generate unique hash for training example"""

        # Combine narrative and source to create unique identifier
        content = f"{narrative}::{source_key}"
        return hashlib.sha256(content.encode()).hexdigest()

    async def is_duplicate(self, content_hash: str) -> bool:
        """Check if this training example already exists"""

        # Check if hash exists in dedup store
        return await self.dedup_store.exists(f"training:{content_hash}")

    async def mark_as_processed(self, content_hash: str) -> None:
        """Mark training example as processed"""

        await self.dedup_store.set(f"training:{content_hash}", "1")
```

### Integration with Formatter

```python
# In TrainingDataFormatter class
async def generate_training_output(
    self,
    narrative: str,
    source_metadata: Dict[str, Any],
    processing_metadata: Dict[str, Any]
) -> bool:
    """Generate training data with deduplication"""

    try:
        # Check for duplicates
        content_hash = self.deduplicator.generate_content_hash(
            narrative,
            source_metadata['key']
        )

        if await self.deduplicator.is_duplicate(content_hash):
            # Skip duplicate
            return False

        # Generate training example (existing code)
        training_example = {...}

        # Append to JSONL
        await self._append_to_jsonl_file(s3_key, jsonl_line)

        # Mark as processed
        await self.deduplicator.mark_as_processed(content_hash)

        return True

    except Exception as e:
        return False
```

---

## Implementation Checklist

### Week 1: Training Data Output
- [ ] Create output module structure
- [ ] Implement `TrainingDataFormatter` class
  - [ ] JSONL generation
  - [ ] Instruction/input generation per record type
  - [ ] Metadata enrichment
  - [ ] S3 key generation (domain-based paths)
  - [ ] JSONL file appending logic
- [ ] Implement `TrainingDeduplicator`
  - [ ] Content hash generation
  - [ ] Duplicate detection
  - [ ] Dedup store integration
- [ ] Implement domain mapping
  - [ ] Map all 6 record types to health domains
- [ ] Implement S3 upload logic
  - [ ] Append to existing files
  - [ ] Create new files when needed
  - [ ] Handle S3 errors gracefully
- [ ] Write unit tests (>80% coverage)
- [ ] Write integration tests
  - [ ] End-to-end: Narrative → JSONL → S3
  - [ ] Verify JSONL format validity
  - [ ] Test deduplication
- [ ] Document JSONL schema

---

## Testing Strategy

### Unit Tests

```python
# tests/test_training_formatter.py
import pytest
import json
from src.output.training_formatter import TrainingDataFormatter

@pytest.mark.asyncio
async def test_generate_instruction_input():
    """Test instruction generation for different record types"""

    formatter = TrainingDataFormatter(None, 'test-bucket')

    instruction, input_text = formatter._generate_instruction_input(
        'BloodGlucoseRecord',
        {'clinical_insights': {'total_readings': 450}}
    )

    assert 'blood glucose' in instruction.lower()
    assert 'analyze' in instruction.lower()
    assert '450' in input_text

@pytest.mark.asyncio
async def test_health_domain_mapping():
    """Test record type to health domain mapping"""

    formatter = TrainingDataFormatter(None, 'test-bucket')

    assert formatter._get_health_domain('BloodGlucoseRecord') == 'metabolic_diabetes'
    assert formatter._get_health_domain('HeartRateRecord') == 'cardiovascular_fitness'
    assert formatter._get_health_domain('SleepSessionRecord') == 'sleep_wellness'
    assert formatter._get_health_domain('StepsRecord') == 'physical_activity'

@pytest.mark.asyncio
async def test_training_file_key_generation():
    """Test S3 key generation"""

    formatter = TrainingDataFormatter(None, 'test-bucket')

    key = formatter._generate_training_file_key('BloodGlucoseRecord')

    assert key.startswith('training/metabolic_diabetes/')
    assert key.endswith('.jsonl')
    assert 'health_journal' in key

@pytest.mark.asyncio
async def test_jsonl_format_validity():
    """Test generated JSONL is valid JSON"""

    formatter = TrainingDataFormatter(None, 'test-bucket')

    narrative = "Test narrative about blood glucose."
    source_metadata = {
        'record_type': 'BloodGlucoseRecord',
        'user_id': 'test_user',
        'correlation_id': 'test-123',
        'bucket': 'test-bucket',
        'key': 'raw/test.avro'
    }
    processing_metadata = {
        'quality_score': 0.95,
        'duration': 1.5,
        'record_count': 100,
        'clinical_insights': {'control_status': 'good'}
    }

    instruction, input_text = formatter._generate_instruction_input(
        source_metadata['record_type'],
        processing_metadata
    )

    training_example = {
        'instruction': instruction,
        'input': input_text,
        'output': narrative,
        'metadata': {...}
    }

    # Verify it's valid JSON
    jsonl_line = json.dumps(training_example)
    parsed = json.loads(jsonl_line)

    assert parsed['instruction'] is not None
    assert parsed['output'] == narrative
    assert 'metadata' in parsed
```

### Integration Tests

```python
# tests/test_training_integration.py
@pytest.mark.integration
@pytest.mark.asyncio
async def test_end_to_end_training_output(minio_client):
    """Test complete training output pipeline"""

    formatter = TrainingDataFormatter(
        s3_client=minio_client,
        bucket_name='health-data',
        training_prefix='training/'
    )

    narrative = (
        "Blood glucose data shows 450 readings with mean of 142 mg/dL. "
        "Control is moderate with 62% time in range."
    )

    source_metadata = {
        'record_type': 'BloodGlucoseRecord',
        'user_id': 'integration_test',
        'correlation_id': 'test-abc-123',
        'bucket': 'health-data',
        'key': 'raw/BloodGlucoseRecord/2025/11/test.avro'
    }

    processing_metadata = {
        'quality_score': 0.95,
        'duration': 2.3,
        'record_count': 450,
        'clinical_insights': {
            'control_status': 'fair',
            'hypoglycemic_events': 3
        }
    }

    # Generate training output
    success = await formatter.generate_training_output(
        narrative, source_metadata, processing_metadata
    )

    assert success is True

    # Verify file was created in S3
    # Key format: training/metabolic_diabetes/2025/11/health_journal_2025_11.jsonl

    # Read the JSONL file
    objects = await list_s3_objects(minio_client, 'health-data', 'training/metabolic_diabetes/')

    assert len(objects) >= 1

    # Download and parse JSONL
    response = await minio_client.get_object(
        Bucket='health-data',
        Key=objects[0]['Key']
    )

    content = await response['Body'].read()
    lines = content.decode().strip().split('\n')

    # Parse last line (most recent entry)
    last_entry = json.loads(lines[-1])

    assert 'instruction' in last_entry
    assert 'output' in last_entry
    assert last_entry['output'] == narrative
    assert last_entry['metadata']['record_type'] == 'BloodGlucoseRecord'
    assert last_entry['metadata']['health_domain'] == 'metabolic_diabetes'
```

---

## Configuration

```python
# src/output/config.py
from pydantic import BaseModel

class TrainingOutputConfig(BaseModel):
    """Configuration for training data output"""

    # S3 settings
    training_data_prefix: str = "training/"
    enable_deduplication: bool = True

    # JSONL settings
    include_metadata: bool = True
    include_clinical_insights: bool = True

    # Instruction templates
    custom_instructions: dict = {}  # Override default instructions per record type
```

---

## Dependencies

### Python Packages
```txt
# Already included in project
pydantic==2.5.0              # Data validation
aioboto3==12.3.0             # S3 uploads
structlog==24.1.0            # Logging
```

### External Services
- MinIO/S3 for training data storage
- SQLite/Redis for deduplication (inherited from Module 1)

---

## Success Criteria

**Module Complete When:**
- ✅ Generates valid JSONL training data
- ✅ Instructions are contextually appropriate per record type
- ✅ Metadata includes all required fields
- ✅ S3 upload working (append to existing files)
- ✅ Deduplication prevents duplicate training examples
- ✅ Files organized by health domain and date
- ✅ Unit tests: >80% coverage
- ✅ Integration tests: End-to-end pipeline passing
- ✅ JSONL files can be loaded by training pipeline
- ✅ Documentation complete with JSONL schema

**Ready for Integration When:**
- ✅ Module 1 can call `generate_training_output()` successfully
- ✅ All 6 record types have domain mappings
- ✅ S3 storage structure is stable
- ✅ Training JSONL format is validated

---

## Integration Points

### **Depends On:**
- **Module 1** (Core Consumer) - Calls this module after successful processing
- **Module 3** (Clinical Processors) - Provides narratives and clinical insights

### **Depended On By:**
- External AI/ML training pipelines (consume JSONL files)

### **Interface Contract:**
```python
# Module 1 calls this after successful processing:
formatter = TrainingDataFormatter(s3_client, bucket_name)

success = await formatter.generate_training_output(
    narrative=processing_result.narrative,
    source_metadata={
        'bucket': message_data['bucket'],
        'key': message_data['key'],
        'record_type': message_data['record_type'],
        'user_id': message_data['user_id'],
        'correlation_id': message_data['correlation_id']
    },
    processing_metadata={
        'duration': processing_result.processing_time_seconds,
        'record_count': len(records),
        'quality_score': validation_result.quality_score,
        'clinical_insights': processing_result.clinical_insights
    }
)
```

---

## Notes & Considerations

1. **JSONL Appending**: S3 doesn't support true append operations. Current implementation downloads, appends, and re-uploads. For high throughput, consider batching or using a local buffer.

2. **File Rotation**: Monthly JSONL files may grow large. Consider daily or weekly rotation for high-volume scenarios.

3. **Instruction Diversity**: Current implementation uses static instructions. Future enhancement: generate diverse instructions for better model generalization.

4. **Metadata Privacy**: Ensure `user_id` and `correlation_id` don't leak PII. Consider hashing or tokenization.

5. **Training Pipeline Integration**: JSONL files are designed for standard fine-tuning tools (Hugging Face Transformers, MLflow, etc.)

---

**End of Module 4 Specification**
